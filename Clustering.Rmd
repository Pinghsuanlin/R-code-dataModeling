---
title: "Ass1"
author: "Ping-Hsuan Lin"
date: "2/8/2022"
output: html_document
---

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
setwd('D:/Spring2022Codes/R')
data <- read.csv(file = 'fastfood_survey.csv',stringsAsFactors = F)
```

1-1. How many variables are included in this dataset?
```{r pressure, echo=FALSE}
str(data)
```

```{r}
head(data)
```
# Create data subset
We will cluster the data based on the eleven fast-food restaurant characteristics that respondents rated the importance of. These are the first eleven variables in the dataset. So, subset the data to only include the first eleven columns of data. Call this data_cluster.
1-2. How many variables does data_cluster have?
```{r}
data_cluster = data[,1:11]
#head(data_cluster)
str(data_cluster)
```

#How many missing value
*Cluster analysis is particularly sensitive to missing values. *
1-3. How many missing values are there for the variable cleanliness?
https://statisticsglobe.com/r-find-missing-values/

https://www.statology.org/number-of-rows-in-r/#:~:text=How%20to%20Count%20Number%20of%20Rows%20in%20R,specific%20column%20of%20data%20frame%20nrow%20%28df%20%5B%21is.na%28df%24column_name%29%2C%5D%29
```{r}
sum(is.na(data_cluster))
sum(is.na(data_cluster['cleanliness']))
```

# Remove missing value
1-4. How many rows of data would be left if rows corresponding to missing values on any of the eleven variables were removed? (Hint: You can use na.omit() but don't overwrite the original data)
```{r}
data_cluster_na = na.omit(data_cluster)
#na.omit returns the object with incomplete cases removed. 
nrow(data_cluster_na)

#Calculate the difference
nrow(data_cluster) - nrow(data_cluster_na)
sum(is.na(data_cluster))
sum(is.na(data_cluster_na))
```
* na.fail returns the object if it does not contain any missing values, and signals an error otherwise.
* na.pass returns the object unchanged.

# Impute missing value
Let us impute the missing values. We are going to make use of the mice package with the default method, predictive mean matching. 
1-5. What is the imputed value of observation 10 for the variable cleanliness?
```{r}
#install.packages('mice') # install mice package if you don't have it
library(mice)
set.seed(1706) #set.seed to get consistent result
data_cluster = mice::complete(mice(data_cluster, use.matcher=T))
#use.matcher = T to reproduce the original behavior

data_cluster[1:10,] #to get the whole picture from observation 1 to 10
data_cluster[10, 'cleanliness'] #observation 10, variable cleanliness
```


```{r}
data_cluster[8,'convenience']
data_cluster[1,'drive_through']
```
# Standardize variables
Cluster analysis is sensitive to the scale of the variables, so we are going to standardize the variables. Use the scale() function as illustrated here:
Note that scale() returns a matrix.
1-6. What is the value of observation 10 for the variable cleanliness?
```{r}
my_data = scale(data_cluster)
my_data[1:10,]
my_data[10, 'cleanliness']
```
#Euclidean distance
Compute the Euclidean distance between all observations in data_cluster. 
2-1. How many elements are in the distance matrix?
```{r}
d = dist(x = my_data,method = 'euclidean') 
#193131 elements
```

# ward.D2 variance & CPC
Conduct a Hierarchical cluster analysis using the method, *'ward.D2'. Plot the dendrogram from this process*. Let us see how well the dendrogram matches true distances. 
2-2. What is the Cophenetic correlation coefficient?

Note: CPC > 0.7 indicates strong fit. 0.3 < CPC < 0.7 indicates moderate fit.
```{r}
clusters = hclust(d = d,method='ward.D2')
#plot(clusters)
cor(cophenetic(clusters),d)
```
# Dendrogram 
2-3. Based on the distances shown in the dendrogram alone, which is the best cluster solution? **NOT SURE** *2 is decided*
```{r}
plot(clusters)

library(dendextend)
plot(color_branches(as.dendrogram(clusters),k = 2,groupLabels = F))

#To clarify the picture, we cut the dendrogram to only display the tree above 5
plot(cut(as.dendrogram(clusters),h=5)$upper)

#Draw rectangless around a two cluster solution
#library(stats)
#rect.hclust(tree=clusters,k = 2,border='tomato') #This returns erro?

```

Highlight n cluster solution using dendextend
```{r}
library(dendextend)
plot(color_branches(as.dendrogram(clusters),k = 6,groupLabels = F))
plot(color_branches(as.dendrogram(clusters),k = 4,groupLabels = F))

plot(0,type='n',axes=FALSE,ann=FALSE)
print(clusters, newpage = F)

library(gridExtra)
library(ggplot2)
library(factoextra)
grid.arrange(fviz_dend(x = clusters, k =2),
             fviz_dend(x = clusters, k =3),
             fviz_dend(x = clusters, k =4)
             )
```


# Select clustering
2-4. If one decided to go with a two-cluster solution, how many observations would be in the smaller of the two clusters? (Save the cluster memberships in an object as you will need it in a later question)
```{r}
h2_segments = cutree(tree = clusters,k=2)
table(h2_segments)

```

2-5. If one decided to go with a three-cluster solution, how many observations would be in the smallest of the three clusters? (Save the cluster memberships in an object as you will need it in a later question)
```{r}
h3_segments = cutree(tree = clusters,k=3)
table(h3_segments)
```

# K-means clustering
2-6. Next, we will cluster the data using k-means. Conduct k-means clustering to generate a two-cluster solution. (arbitrary assignment) 

*Since the choice of initial solution in k-means has a random element, use a seed of 1706. Set max iterations to 100. Do not set nstart. (Save the cluster memberships in an object as you will need it in a later question). How many observations are in the smaller cluster?

```{r}
set.seed(1706)
km2 = kmeans(x = my_data,centers = 2,iter.max=100)
table(km2$cluster)
```

2-7. Run another k-means clustering, but this time to generate a three-cluster solution. As above, use a seed of 1706 and set max iterations to 100. Do not set nstart. (Save the cluster memberships in an object as you will need it in a later question)
How many observations are in the smallest cluster?

```{r}
set.seed(1706)
km3 = kmeans(x = my_data,centers = 3,iter.max=100)
table(km3$cluster)
```

# Determine cluster solutions
#Total within sum of squares plot
2-8. In the above k-means analyses, we set the number of clusters expected. Now, let us examine a data driven approach to determining the number of clusters. Compute the total within cluster sum of squares for cluster solutions from 2 to 10. Use a seed of 1706. Do not set nstart. What is the total within cluster sum of squares for a three-cluster solution?
```{r}
within_ss = sapply(1:10,FUN = function(x){
  set.seed(1706)
  kmeans(x = my_data, centers = x,iter.max = 100)$tot.withinss})
print(within_ss[3]) 
print(within_ss)
```

2-9. For the three-cluster solution, what is the ratio of between sum of squares and total sum of squares? (Express as a decimal.)
```{r}
km3$betweenss / km3$totss
#The total sum of squares is the sum of total within-cluster sum of squares and the between cluster sum of squares
km3$totss == km3$betweenss + km3$tot.withinss
```

2-10. Construct a line graph of clusters (on x-axis) against total within cluster sum of squares (on y-axis). Based on this chart, which of the following are good cluster solutions?
```{r}
library(ggplot2)
within_ss_a = sapply(1:10,FUN = function(x){
  set.seed(1706)
  km =kmeans(x = my_data,centers = x, iter.max = 100)
  km$betweenss / km$totss})
  
ggplot(data=data.frame(cluster = 1:10,within_ss_a),aes(x=cluster,y=within_ss_a))+
  geom_line(col='steelblue',size=1.2)+
  geom_point()+
  scale_x_continuous(breaks=seq(1,10,1))
#From the plot, 2 or 3 are both good cluster solutions.
```
#Ratio plot
```{r}
ratio_ss = sapply(1:10,FUN = function(x) {
  set.seed(1706)
  km = kmeans(x = my_data,centers = x,iter.max = 1000,nstart = 25)
  km$betweenss/km$totss} )

ggplot(data=data.frame(cluster = 1:10,ratio_ss),aes(x=cluster,y=ratio_ss))+
  geom_line(col='steelblue',size=1.2)+
  geom_point()+
  scale_x_continuous(breaks=seq(1,10,1))
```

# Silhouette method

2-11. Next, let us examine the Silhouette method, another data driven approach to choosing number of clusters. What is the average silhouette width for a 2 cluster solution? Use pam() from library(cluster) to compute silhouette width.

Note: -1 <= s(i) <= 1
s(i) = 1, implies i is in the right cluster; 0, between clusters; -1 should be in the nearest cluster
```{r}
library(cluster)
pam(my_data,k = 2)$silinfo$avg.width
```
2-12. What is the average silhouette width for a 3 cluster solution?
```{r}
pam(my_data,k = 3)$silinfo$avg.width

```

2-13. Examine average silhouette width for other cluster solutions. Based on this criterion, which is the best cluster solution?
```{r}
library(cluster)
silhoette_width = sapply(2:10,FUN = function(x) pam(x = my_data, k = x)$silinfo$avg.width)

ggplot(data=data.frame(cluster = 2:10, silhoette_width), aes(x=cluster,y=silhoette_width))+
  geom_line(col='steelblue',size=1.2)+
  geom_point()+
  scale_x_continuous(breaks=seq(2,10,1))
#The best cluster solution is 2
```
#Model-based cluster
#bic
2-14. Now, we will make use of a Model-based clustering technique. Use Mclust() from library(mclust) to cluster the data. How many clusters has the model decided to group the data into?

```{r}
#install.packages('mclust')
library(mclust)
clusters_mclust = Mclust(my_data)
#summary(clusters_mclust)

#The optimal cluster solution is the one that perform best on BIC and log.likelihood
#Some R functions including mclust generate negative BIC, so you need to reverse the sign of bic
mclust_bic = sapply(1:10,FUN = function(x)- Mclust(my_data,G=x)$bic)
mclust_bic #from here, we see cluster 3 has the lowest

clusters_mclust_3 = Mclust(my_data,G=3)
summary(clusters_mclust_3)
```

#plot of bic
```{r}
ggplot(data=data.frame(cluster = 1:10,bic = mclust_bic),aes(x=cluster,y=bic))+
  geom_line(col='steelblue',size=1.2)+
  geom_point()+
  scale_x_continuous(breaks=seq(1,10,1))
```
#size of clusters
2-15. Now, use model-based clustering to force a two-cluster solution. (Save the cluster memberships in an object as you will need it in a later question). How many observations are in the smaller cluster?
```{r}
m2_clusters = Mclust(data = my_data,G = 2)

m2_segments = m2_clusters$classification
table(m2_segments)
```

#compare clustering 
2-16. Compare the two-cluster solutions obtained from hierarchical cluster to k-means. Specifically, compare the cluster assignments for hierarchical cluster analysis to k-means for the two-cluster solution. For how many observations do the cluster assignments differ?

Note: Since we are unsure how group label is assigned to each data point, we will use this rule: The number of cases that differ will be the minimum of a) the number of cases that match and b) the number of cases that dont match
```{r}
set.seed(1706)
#km2 = kmeans(x = my_data,centers = 2,iter.max=100)
table(km2$cluster)
#h2_segments = cutree(tree = clusters,k=2)
table(h2_segments)

sum(km2$cluster == h2_segments)
```


```{r}
h2_segments = cutree(tree = clusters,k=2)
table(h2_segments)
```

2-17. Compare the two-cluster solutions for k-means to Model-based clustering. Specifically, compare the cluster assignments for k-means to Model-based clustering. For how many observations do the cluster assignments differ?
```{r}
sum(m2_segments == km2$cluster)
```
#Combine cluster membership with original dataset
Third cluster
```{r}
set.seed(1706)
km3 = kmeans(x = my_data,centers = 3,iter.max=100)
table(km3$cluster)
#Combine cluster membership with original dataset
data2 = cbind(data, kmc=km3$cluster)

```

#profile segments by needs
3-1~4. Compared to other clusters, Cluster 1 has the lowest value for:
```{r}
library(dplyr)
data2 %>%
  select(speed_of_service:income,kmc)%>%
  group_by(kmc)%>%
  summarize_all(function(x) round(mean(x,na.rm=T),2))%>%
  data.frame()
#compare and choose out the lowest in cluster1
```
#Distribution of factor demographic variables
Demographic makeup of the customers that belong to each cluster or market segment.
```{r}
#To examine distributions of factor demographic variables, you could use the table function. E.g.,
#table(data2$kmc,data2$demographicVariable)


#Since segment sizes are different, you could examine percentage of each group in the segment by using prop.table as illustrated here:
#round(prop.table(table(data2$kmc,data2$demographicVariable),1),2)*100

```

3-5~7.Compare to other clusters, Cluster1, 2, 3
```{r}
lapply(1:22,function(x) round(prop.table(table(data2$kmc,data2[,x]),1),2)*100)
```

3-6. Spend the most when eating out; Has the most number of children; 
```{r}
table(data2$kmc,data2$number_children)
table(data2$kmc,data2$dollars_avg_meal)
```
