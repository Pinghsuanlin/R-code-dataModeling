```{r}
setwd('D:/Spring2022Codes/R')
data <- read.csv(file = 'baby_reviews.csv',stringsAsFactors = F)
```

1-1. How many reviews does this dataset contain?
```{r}
str(data)
```

1-2. What is the average review rating?
```{r}
mean(data$review_rating)
```
#Count the number of character
1-3. nchar() is a handy function for counting the number of characters in text. What is the average number of characters in a review?
```{r}
mean(nchar(data$review))
```
#Pearson correlation
1-4. Examine the relationship between review length (measured by number of characters) and rating. Now, indicate whether the following statement is True or False.

Notes:*cor.test() Test for association between paired samples, using one of Pearson's product moment correlation coefficient*
Here, the results is the same as cor()

Greater the length of the review, better the rating.
```{r}
cor.test(nchar(data$review), data$review_rating)
#cor(nchar(data$review), data$review_rating) #result is the same
```

#Text Analysis
#Examine by characters, words, sentences 
1-5. The stringr library has a number of handy text search functions capable of both literal search and pattern matching. The sample code that follows specifies a pattern to identify a word and str_count to count the number of such words in a set of text.

Using the code given, find the median number of words in a review?

1-6. How many words are in the longest review?
1-7. How many words are in the shortest review?

```{r}
library(stringr)
summary(str_count(string = 'Hmm, how many words are in this sentence?',pattern = '\\S+')) #8 words in total
str_count(string = data$review[2],pattern = "[A-Za-z,;'\"\\s]+[^.!?]*[.?!]") #4 sentences
nchar(data$review[2]) #168 characters

summary(str_count(string = data$review,pattern = '\\S+'))

```
#Common words
1-8. Next, let us examine which words are used most frequently in the review. For this, you can use relevant functions from *library(tidytext)*. Which of the following words are in the Top 10 list?

```{r}
#install.packages('tidytext')
library(dplyr); library(tidytext); library(magrittr)
data%>%
  unnest_tokens(input = review, output = word)%>%
  select(word)%>%
  group_by(word)%>%
  summarize(count = n())%>%
  ungroup()%>%
  arrange(desc(count))%>%
  top_n(10)
```
#w/o stop words: Common words
1-9. Now, let us construct a Top 10 list after excluding stop words. For the purpose of this question use the set of stop words from library(tidytext): stop_words 

Which of the following words are in this Top 10 list? (Are you surprised?)

```{r}
data%>%
  unnest_tokens(input = review, output = word)%>%
  select(word)%>%
  anti_join(stop_words)%>%
  group_by(word)%>%
  summarize(count = n())%>%
  ungroup()%>%
  arrange(desc(count))%>%
  top_n(10)
```
#Tokenize
*tidytext::unnest_tokens default token is words*
2-1. Let us use the dplyr and tidytext packages to explore the words used in the review. Use the unnest_tokens function from tidytext to tokenize the reviews and the following dplyr functions to organize the data: select, group_by, ungroup and count.

What is the total number of words in all the reviews?

```{r}
data %>%
  select(id,review)%>%
  group_by(id)%>%
  unnest_tokens(output = word,input=review)%>%
  ungroup()%>%
  count()

#or
data %>%
  select(id,review)%>%
  group_by(id)%>%
  unnest_tokens(output = word,input=review)%>%
  ungroup()%>%
  #group_by(id)%>%
  summarize(count = n())
```

#Categorize: bing, nrc
#Binary Sentiment Lexicons
#bing
2-2. Now, let us explore valence of the words used in reviews. Use the 'bing' dictionary to classify words in reviews into positive and negative. The bing dictionary of words can be accessed using tidytext::get_sentiments('bing') or bing.csv file accompanying this assignment. 

## by number
What is the total number of positive words in all the reviews?
```{r}
#get_sentiments('bing')%>%
  #group_by(sentiment)%>%
  #count()

data%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = review)%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(sentiment)%>%
  count()
```
## by proportion
2-3. Among all the review words categorized as either positive or negative (using the ‘bing’ dictionary), what proportion are positive?
```{r}
data %>%
  select(id,review)%>%
  group_by(id)%>%
  unnest_tokens(output=word,input=review)%>%
  ungroup()%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(sentiment)%>%
  summarize(n = n())%>%
  mutate(proportion = n/sum(n))
```
## by review & filter
2-4. Now, let us examine the proportion of positive words in reviews for each review rating. Of the five possible review ratings, which has the highest proportion of positive words?
```{r}
data %>%
  select(id,review,review_rating)%>%
  group_by(id, review_rating)%>%
  unnest_tokens(output=word,input=review)%>%
  ungroup()%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(review_rating,sentiment)%>%
  summarize(n = n())%>%
  mutate(proportion = n/sum(n)) %>%
  filter(sentiment == 'positive')
```

#nrc Emotion
categorized by emotion
Note: There's also nrc sentiment
nrc Sentiment: It is a binary sentiment lexicon that categorizes words as +1 or -1

## load nrc Lexicon from github content
Next, let us examine the emotions expressed in the reviews using the "nrc" dictionary. You can access the "nrc" dictionary from nrc.csv file accompanying this assignment. 
```{r}
#load nrc Lexicon from github content
nrc = read.table(file = 'https://raw.githubusercontent.com/pseudorational/data/master/nrc_lexicon.txt',
                 header = F,
                 col.names = c('word','sentiment','num'),
                 sep = '\t',
                 stringsAsFactors = F)
nrc = nrc[nrc$num!=0,]
nrc$num = NULL

#list of emotions included in this lexicon
nrc %>%
  group_by(sentiment)%>%
  count()
```
2-5. How many words reflect surprise?
2-6. How many words reflect anticipation?
```{r}
data%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = review)%>%
  inner_join(nrc)%>%
  group_by(sentiment)%>%
  count()%>%
  arrange(desc(n))
```
# afinn sentiment
Note: afinn lexicon scores each work based on "the extent" to which it's positive or negative

The 'afinn' dictionary scores the sentiment of words. Use this dictionary to determine the sentiment of each review. See class R code on how to access 'afinn' dictionary. 
## Load afinn
```{r}
#Load afinn
afinn = read.table('https://raw.githubusercontent.com/pseudorational/data/master/AFINN-111.txt',
                   header = F,
                   quote="",
                   sep = '\t',
                   col.names = c('word','value'), 
                   encoding='UTF-8',
                   stringsAsFactors = F)

```

2-7. What is the minimum sentiment score?
2-9. What is the average sentiment score (based on the ‘afinn’ dictionary)?
```{r}
data %>%
  select(id,review)%>%
  group_by(id)%>%
  unnest_tokens(output=word,input=review)%>%
  inner_join(afinn)%>%
  summarize(reviewSentiment = mean(value))%>%
  ungroup()%>%
  summarize(min=min(reviewSentiment),
            max=max(reviewSentiment),
            median=median(reviewSentiment),
            mean=mean(reviewSentiment))
```

2-8. Which of the following review ids have the lowest sentiment score? (If you are curious, you may also want to take a look at the review to understand the reason for the low sentiment score)
```{r}
data %>%
  select(id,review)%>%
  group_by(id)%>%
  unnest_tokens(output=word,input=review)%>%
  inner_join(afinn)%>%
  summarize(reviewSentiment = mean(value))%>%
  ungroup() %>%
  arrange(reviewSentiment)%>%
  top_n(-25)
```
# create a corpus
Preprocess the corpus of reviews using functions from library(tm). Specifically,

1. Create a corpus from the variable 'review'
2. Use tm_map to
(a) transform text to lower case,
(b) remove punctuation,
(c) remove English stopwords using the following dictionary tm::stopwords('english)
(d) remove whitespace
3. Create a dictionary
4. Use tm_map to stem words
5. Create a DocumentTermMatrix
```{r}
#install.packages('tm')
library(tm)
#1. Create a corpus from the variable 'review'
corpus = Corpus(VectorSource(data$review))
```
## clean text
```{r}
#(a) transform text to lower case,
corpus = tm_map(corpus,FUN = content_transformer(tolower))

#(b) remove punctuation,
corpus = tm_map(corpus,FUN = removePunctuation)

#(c) remove English stopwords using the following dictionary tm::stopwords('english)
corpus = tm_map(corpus,FUN = removeWords,c(stopwords('english')))

#(d) remove whitespace
corpus = tm_map(corpus,FUN = stripWhitespace)

```

## create a dictionary
similar words have been grouped together
```{r}
#3. Create a dictionary
dict = findFreqTerms(DocumentTermMatrix(Corpus(VectorSource(data$review))),lowfreq = 0)
dict_corpus = Corpus(VectorSource(dict))
```

## stem document
```{r}
#4. Use tm_map to stem words
corpus = tm_map(corpus,FUN = stemDocument)
```

#create a document term matrix (tokenize)
3-1. How many terms does the document term matrix contain?
```{r}
#5. Create a DocumentTermMatrix
dtm = DocumentTermMatrix(corpus); dtm
```

3-2. Inspect document 100 of the document term matrix. How many times does 'amazon' appear in this document?
```{r}
inspect(dtm[100,'amazon'])
#terms: 1 ~~ 1 time
```

#remove sparse terms
When facing the curse of dimensionality where there're more variables than observations, remove infrequently occurring terms.
3-3. Now, let us reduce the number of terms to a more reasonable number by only keeping terms that appear in at least 10% of documents. Save the result as 'xdtm'. How many terms remain after removing sparse terms?
```{r}
#remove sparse term
xdtm = removeSparseTerms(dtm,sparse = 0.9); xdtm
```

#complete stems
note so that term can become column names
3-4. Transform the document term matrix, xdtm created in the previous question into a data frame. Use stemCompletion() to complete stemmed words by selecting the most prevalent match. In the resulting data frame, which term appears most frequently?
```{r}
#turn dtm into df
xdtm = as.data.frame(as.matrix(xdtm))

#complete stemmed words by selecting the most prevalent match
colnames(xdtm) = stemCompletion(x = colnames(xdtm),
                                dictionary = dict_corpus, type='prevalent')

colnames(xdtm) = make.names(colnames(xdtm))

sort(colSums(xdtm),decreasing = T)
```

3-5. Attach the column containing the review rating to the dataframe created in the previous question. Which is the third (3rd) most frequently occurring word among reviews with a rating of 5?

```{r}
trial = cbind(review_rating = data$review_rating,xdtm)

trial1 <- trial %>%
  filter(review_rating==5)

colnames(trial1) = stemCompletion(x = colnames(trial1), dictionary = dict_corpus, type='prevalent')

colnames(trial1) = make.names(colnames(trial1)); colnames(trial1)

sort(colSums(trial1),decreasing = T)
```

#tfidf: Term Frequency-Inverse Document Frequency Weighting
#document term matrix

Now, let us use data on word frequencies to predict review rating. Split the dataset containing review rating and term frequencies into train and test samples. Use sample() to create a train sample with 70% of the data and a test sample with the remaining 30%. Use a seed of 1031. For a dataset called, baby_data, the following code will create the train and test samples.

```{r}
dtm_tfidf = DocumentTermMatrix(x=corpus,
                               control = list(weighting=function(x) weightTfIdf(x,normalize=F)))
xdtm_tfidf = removeSparseTerms(dtm_tfidf,sparse = 0.9)
xdtm_tfidf = as.data.frame(as.matrix(xdtm_tfidf))

baby_data = cbind(review_rating = data$review_rating,xdtm)
baby_data_tfidf = cbind(review_rating = data$review_rating,xdtm_tfidf)
```

#Build predictive model
#Split data (TF)

4-1. How many rows are in the test sample?
```{r}
set.seed(1031)
split = sample(1:nrow(baby_data),size = 0.7*nrow(baby_data))
train = baby_data[split,]
test = baby_data[-split,]
nrow(test)
```
#CART
Use a CART model to predict review_rating using all other variables, i.e., term frequencies. For the CART model, use rpart(). Now, indicate whether the following statement is True or False?.

4-2. Based on results of the CART model (and all else being equal), reviews that contain the term 'love' are rated higher than those that don't contain the term 'love'?
*True*

4-3. Based on results of the CART model, reviews that contain the term 'easier' are rated lower than those that don't contain the term 'easier'?
*False*

4-4. Based on results of the CART model, reviews that contain the term 'perfect' are rated lower than those that don't contain the term 'perfect'?
*False*
```{r}
library(rpart); library(rpart.plot)
tree = rpart(review_rating~.,train)
rpart.plot(tree)
```
#Linear Regression
Use a linear regression to predict review_rating using all other variables, i.e., term frequencies.

4-5. 
Examine the results. From an earlier section, recall the most frequently occurring term in the document term matrix after removing sparse terms. *use* 
Locate the most frequently occurring term in review in the regression results. Is this term predictive of review_rating?
*False* Bc p-value > 0.05
```{r}
reg = lm(review_rating~.,train)
summary(reg)
```
## RMSE for CART
4-6. What is the RMSE (root mean squared error) of the CART model on the test set?
```{r}
pred_tree = predict(tree,newdata=test)
rmse_tree = sqrt(mean((pred_tree - test$review_rating)^2)); rmse_tree
```
## RMSE for lr
4-7. What is the RMSE of the linear regression model on the test set?
```{r}
pred_reg = predict(reg, newdata=test)
rmse_reg = sqrt(mean((pred_reg-test$review_rating)^2)); rmse_reg
```

