---
title: "Ass4"
author: "Ping-Hsuan Lin"
date: "4/11/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ensure you are using R Version 4.1.2, forecast version 8.15 and xts version 0.12.1. 
```{r}
R.Version()
#install.packages('forecast')
packageVersion(pkg = 'forecast')
#install.packages('xts')
packageVersion(pkg = 'xts') 
```

#Packages Used
```{r}
library(ggplot2);library(ggthemes);library(gridExtra)  # For plots 
library(quantmod);library(xts);library(zoo) # For using xts class objects
library(forecast) # Set of forecasting functions
library(fpp); library(fpp2) # Datasets from Forecasting text by Rob Hyndman
library(tseries) # for a statistical test
library(dplyr) # Data wrangling
```
#data structure: class()
1-1. Read the file goog.RDS using the function readRDS(). If goog.RDS is in your working directory, run readRDS to read the data in and assign it to goog:
What type of data structure is goog?
```{r}
setwd('D:/Spring2022Codes/R')
goog = readRDS('goog.RDS')
#goog = readRDS('dD:/Spring2022Codes/R/goog.RDS')
class(goog)
#xts = eXtensible Time Series 
```

```{r}
summary(goog)
```
```{r}
head(goog)
```

#Explore data
**1-2. What was google stock price for June, 2010?
```{r}
goog["2010", ]
```


1-3. Using the monthly stock price for google, what is the average stock price for the year 2010?
```{r}
mean(goog["2010", ])
```

#Create a ts object
#size: length()
*annual: 1(default); quarterly: 4; monthly: 12; weekly: 52*
**1-4. How many months of data are included in this dataset?
```{r}
#Create a ts object
sales_ts = ts(data=goog,frequency = 12); sales_ts
#sales_ts = ts(data=goog,start = 2010, frequency = 12)

#count
length(sales_ts)

length(goog)
```
#Lagged correlations
## one-month lag
1-5. With time-series data, the past is often a good predictor of the future. Let us see if this is true for our data. What is the correlation between google stock price and one-month lagged stock price? 

You can *use lag() to obtain a one-month lag for google stock price*. When computing correlation with cor(), be sure to set *use='complete.obs'*.

```{r}

cor(goog, stats::lag(goog, k = 1), use = 'complete.obs')
```

#convert to a ts object with starting time
#Split the data by date

1-6. In order to have access to a wider array of forecasting models, we will convert the data to a "ts" data type. Also, we will split the data into a train and test sample, using the train sample to estimate a model and the test sample to evaluate it. We will used data from Jan, 2007 to Dec, 2015 for the train sample and the rest for the test sample. The code below will convert goog to a “ts” object and split the data.

How many months of data does train contain?
```{r}
google = ts(goog,start=c(2007,01),frequency=12)

train = window(google,start=c(2007,01),end=c(2015,12))
test = window(google,start=c(2016,01),end=c(2018,10))

length(train) #therefore, we will construct forecasts for 108 periods (ie. 108 is the length of test)
```
#Strongest Autocorrelation
## ggAcf()
1-7. Autocorrelation examines correlation of a variable and its lagged values. Construct a plot of autocorrelations for train using ggAcf() from the forecast package. Which lag has the strongest autocorrelation?
```{r}
ggAcf(train)
 
#acf(train, plot = F) #examine the top few autocorrelations
```
#Simple forecasting methods
## Average method: meanf()
*Average is used as the baseline in linear regression*
## point forecast: window()

**2-1. A very simple prediction, often the baseline in linear regression, is to use the average. Use the average to make a prediction for the stock price over the 34 months of the test sample. Let's call this average_model. What is the point forecast of the stock price for October 2018?
```{r}
length(test)
#model and forecast
average_model = meanf(train,h = 108) #h is length of test
average_model

#stats::window(average_model$mean,c(2018,10))
window(average_model$mean,c(2018, 10))
```
## accuracy(): rmse
2-2. Let us examine the accuracy of the above prediction from average_model on the train sample.
Specifically, what is the RMSE of the prediction in the train sample? Hint: Use accuracy() from library(forecast)

```{r}
accuracy(average_model)
```

## rmse on the test sample
**2-3. What is the RMSE of the average_model on the test sample?

```{r}
accuracy(average_model, x = google) #put x to include test set
```

##Naive method: naive()
*Also called 'random walk forecasts'. Future will be the same as the last observation.*
2-4. Next, let us examine another simple prediction, one that assumes the future will be the same as the last observation. Let’s call this naive_model. Use naive_model to construct a forecast for stock price over the next 34 months of the test sample. What is the point forecast of the stock price for October 2018?

```{r}
naive_model = naive(train,h=108)
naive_model$mean
#last(train) #For a naive model, the forecastis the same as the last observation in the train sample

window(naive_model$mean,c(2018, 10))
```

2-5. What is the RMSE of the naive_model on the test sample?

```{r}
accuracy(naive_model,x=google)
```

##Seasonal naive method: snaive()
*Forecast is equal to the last observed value but for highly seasonal data*

```{r}
seasonal_naive_model = snaive(train,h=108); seasonal_naive_model
seasonal_naive_model$mean

#accuracy
accuracy(seasonal_naive_model, x = google); accuracy

```

#Exponential smoothing models
*Forecasts are weighted averages of past observations with the weights decaying exponentially such that recent observations get weighted more than distant observations*

There are 9 exponential smoothing methods: Trend(none, additive, additive damped), Seasonal components(none, additive, multiplicative), Errors(addictive, multiplicative)

#ETS models: 
*forecast::ets(data, model = 'error, trend, seasonality')*
## ETS:AAA
There are a number of exponential smoothing models that differ in how they handle errors, trend, and seasonality.

3-1. The errors of ets_model is?
3-2. The trend for ets_model is?
```{r}
# Let us fit an exponential smoothing model using the following function:
ets_model = ets(train,model = 'AAA')
#Call this ets_model
```

## ETS: Automatic selection
When only the time series is specified, and all other argument are left at default values, then ets() will automatically select the best model based on AICs.
```{r}
ets_auto = ets(train)
summary(ets_auto)
#ANN is chosen= Addictive error, None trend and none seasonality

#forecast
ets_auto_forecast = forecast(ets_auto, h = 108)
#accuracy
accuracy(ets_auto_forecast, x = google); accuracy #rmse for test set 
```

## AIC
3-3. What is AICc for ets_model?
*lower AIC is better*
AIC score is only useful comparing with other AIC.
```{r}
summary(ets_model)
```
#White noise residual
## Ljung-Box test: checkresiduals()

**3-4. Do the residuals look like white noise? 
*No, bc you have a significant Ljung-Box test* To answer this question, examine an Acf() or ggAcf() plot and the result of the Ljung-Box test.

If you have a good fit for the signal, the residuals should be white noise  (or independent when their distributions are normal). 
H0:	The data are random. (The residuals are independently distributed.)
Ha:	The data are not random. (The residuals are not independently distributed; they exhibit serial correlation.)

*We want to 'fail to reject H0; we want residual to be independent'; we want p-value > 0.05*

```{r}
ggAcf(x = google)
acf(google)
checkresiduals(ets_model)
```
Side-Notes:*Ljung-Box test*
Autocorrelation plots are one common method test for randomness. The Ljung-Box test is based on the autocorrelation plot. However, instead of testing randomness at each distinct lag, it tests the "overall" randomness based on a number of lags. For this reason, it is often referred to as a "portmanteau" test.

## ETS forecast
3-5. Use ets_model to construct a forecast for stock price over the next 34 months of the test sample. What is the point forecast of the stock price for October 2018?

```{r}
ets_aaa_forecast = forecast(ets_model,h=108)
ets_aaa_forecast$mean
window(ets_aaa_forecast$mean, c(2018, 10))
```
## ets accuracy
3-6. What is the RMSE of the ets_model on the test sample?
*RMSE lower or higher? It depends. The lower the RMSE, the better a given model is able to “fit” a dataset. The range of the dataset you’re working with is important in determining whether or not a given RMSE value is “low” or not.
```{r}
accuracy(ets_aaa_forecast, x = google)
```

# ARIMA
*Exponential Smoothing models aim to describe tend and seasonality; ARIMA aims to describe autocorrelations*

## Auto model selection: auto.arima()
Now, let’s use an ARIMA model. Since, there are a large number of parameters with which to define the ARIMA model, let use the auto.arima() function to automatically determine the best parameters. Use the defaults for auto.arima(). For instance, if your dataset is called train, run auto.arima(train). Call this auto_arima_model. 

**4-1. How many ordinary autoregressive lag variables have been used in auto_arima_model?
 *0*
*p*

4-2. What is the number of ordinary differences used in auto_arima_model? *1*
*d*

**4-3. How many ordinary moving average lags have been used in auto_arima_model? 
*q*
*0*

**4-4. How many seasonal autoregressive lag variables have been used in auto_arima_model? 
*P*
*0*

```{r}
auto_arima_model = auto.arima(train); auto_arima_model
# A nonseasonal ARIMA model is classified as an "ARIMA(p,d,q)" model, where:
# p is the number of autoregressive terms,
# d is the number of nonseasonal differences needed for stationarity (ie. the number of ordinary differences)
# q is the number of lagged forecast errors in the prediction equation. (ie. ordinary moving average lags; seasonal autoregressive lag)
```

**4-5. Do the residuals look like white noise? To answer this question, examine an Acf() or ggAcf() plot and the result of the Ljung-Box test.
*Yes* (bc p-value >0.05, it's randomly distributed)
```{r}
#ACF is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values. We plot these values along with the confidence band and tada! We have an ACF plot. In simple terms, it describes how well the present value of the series is related with its past values.
#https://medium.com/analytics-vidhya/interpreting-acf-or-auto-correlation-plot-d12e9051cd14
#acf(train)
#ggAcf(train)
checkresiduals(auto_arima_model)
#For white noise series, we expect each autocorrelation to be close to zero. 
#https://otexts.com/fpp2/wn.html
#https://pkg.robjhyndman.com/forecast/reference/checkresiduals.html
```

4-6. Use auto_arima_model to construct a forecast for stock price over the next 34 months of the test sample. What is the point forecast of the stock price for October 2018?

```{r}
auto_arima_model_forecast = forecast(auto_arima_model, h = 108); auto_arima_model_forecast

auto_arima_model_forecast$mean
window(auto_arima_model_forecast$mean, c(2018, 10))
```

4-7. What is the RMSE of auto_arima_model on the test sample?

```{r}
accuracy(auto_arima_model_forecast, x = google)
```

#Choose optimal lambda: BoxCox.lambda
**4-8. Let us see if we can improve our ARIMA model by a variance stabilizing transformation.
BoxCox.lambda() is a handy function for identifying the optimal value of lambda to stabilize variance. What is the optimal value of lambda?
```{r}
train_box = Arima(y=train, lambda = BoxCox.lambda(train)); train_box
```

4-9. What is the AICc for arima_model?

```{r}
#Rather than using auto.arima(), let us specify an ARIMA model as follows:
arima_model = Arima(train,order = c(1,1,1),seasonal = c(3,1,0),lambda=BoxCox.lambda(train)); arima_model
#Call this arima_model
```

**4-10. Examine the results of Ljung-Box test (using the default of 24 lags) to see if the residuals resemble white noise.
*yes*

```{r}
checkresiduals(arima_model)
```

4-11. Use arima_model to construct a forecast for stock price over the next 34 months of the test sample. What is the point forecast of the stock price for October 2018?

```{r}
arima_model_forecast  = forecast(arima_model, h = 108); arima_model_forecast 

window(arima_model_forecast$mean, c(2018, 10))
```

4-12. What is the RMSE of arima_model on the test sample?

```{r}
accuracy(arima_model_forecast, x = google)
```

